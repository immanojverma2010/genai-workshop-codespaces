{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEeUZ1r79kr0"
      },
      "source": [
        "# Setup Environment\n",
        "The following code loads the environment variables required to run this notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iqND-S5P9kr0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1a16d58cdbea4453a29c61b5a32dd1e6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Please enter the Workshop Key provided by the instructor:'), HBox(children=(Text(vâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "workshop expired\n",
            "workshop expired\n"
          ]
        }
      ],
      "source": [
        "FILE=\"GenAI Lab 1 and 2\"\n",
        "\n",
        "# ! pip install -qqq git+https://github.com/elastic/notebook-workshop-loader.git@main\n",
        "from notebookworkshoploader import loader\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "if os.path.isfile(\"../env\"):\n",
        "    load_dotenv(\"../env\", override=True)\n",
        "    print('Successfully loaded environment variables from local env file')\n",
        "else:\n",
        "    loader.load_remote_env(file=FILE, env_url=\"https://notebook-workshop-api-voldmqr2bq-uc.a.run.app\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sm8uPLZxNOF3"
      },
      "source": [
        "# Lab 1-1: Using Transformer Models\n",
        "\n",
        "In this lab we will\n",
        "* Intro to Python Notebooks - Hello World, importing python libraries\n",
        "* Caching the download of a smaller LLM\n",
        "* Using a basic transformer models locally\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqlXsCGMN6TC"
      },
      "source": [
        "## Step 1: Hit play on the next code **sample**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "r0WRcBgDNOF3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello World\n"
          ]
        }
      ],
      "source": [
        "print(\"Hello World\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P85HsyKmNOF4"
      },
      "source": [
        "## Step 2: Use ! to execute a shell command"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "sfW6H9meNOF4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The shell thinks the Current Directory is: /workspaces/genai-workshop-codespaces/notebooks\n"
          ]
        }
      ],
      "source": [
        "! echo \"The shell thinks the Current Directory is: $(pwd)\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSXuTJelNOF5"
      },
      "source": [
        "## Step 3: Environment setup\n",
        "\n",
        "First let us import some Python libraries we'll use in the first lab module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FivGgCcXNOF5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "github codespaces has pre-installed these libraries\n"
          ]
        }
      ],
      "source": [
        "! ## pip install -qqq --upgrade pip\n",
        "# ! pip install -qqq torch==2.1.0\n",
        "# ! pip install -qqq --upgrade transformers==4.36.2\n",
        "# ! pip install -qqq python-dotenv==1.0.0\n",
        "# ! pip install -qqq tiktoken==0.5.2 cohere==4.38 openai==1.3.9          ## for later in the lab\n",
        "! echo \"github codespaces has pre-installed these libraries\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kk2GXx6ANOF5"
      },
      "source": [
        "## Step 4: Utility functions\n",
        "Some utility functions that are good to keep on hand"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0kVJFIJxNOF5"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "# pretty printing JSON objects\n",
        "def json_pretty(input_object):\n",
        "  print(json.dumps(input_object, indent=4))\n",
        "\n",
        "\n",
        "import textwrap\n",
        "# wrap text when printing, because colab scrolls output to the right too much\n",
        "def wrap_text(text, width):\n",
        "    wrapped_text = textwrap.wrap(text, width)\n",
        "    return '\\n'.join(wrapped_text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIPsPhYuNOF5"
      },
      "source": [
        "## Step 5: Download sentiment analysis model from HuggingFace\n",
        "\n",
        "We'll use the Huggingface Transformer library to download and ready an Open Source model called DistilBERT which can be used for sentiment analysis.\n",
        "\n",
        "* Details of the model can be found on its [Hugging Face Page](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n",
        "* This model is pretrained to determine if an input text is of *POSITIVE* or *NEGATIVE* sentiment and makes a good intro example to AI models.\n",
        "* Note we are caching the model files in a folder called ```llm_download_cache``` which will help us not have to re-download the files again within the connection to this runtime. You can see the download in the filesystem (using the left hand side menu)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Rg4o4viHNOF5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/codespace/.python/current/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/home/codespace/.python/current/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.00453948974609375,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "Downloading tokenizer_config.json",
              "rate": null,
              "total": 48,
              "unit": "B",
              "unit_divisor": 1000,
              "unit_scale": true
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1233ffd8bf834d18bc28e0b6406c11a6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.0042455196380615234,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "Downloading vocab.txt",
              "rate": null,
              "total": 231508,
              "unit": "B",
              "unit_divisor": 1000,
              "unit_scale": true
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "83888797ef4247c487ae977189a8488e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.004724740982055664,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "Downloading config.json",
              "rate": null,
              "total": 629,
              "unit": "B",
              "unit_divisor": 1000,
              "unit_scale": true
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e2eba4632c5a4259a1cc29a7daa4a8ad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.004121541976928711,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "Downloading model.safetensors",
              "rate": null,
              "total": 267832558,
              "unit": "B",
              "unit_divisor": 1000,
              "unit_scale": true
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d945a4cc106f482e8763644e7c67e4f1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "import tqdm\n",
        "import json\n",
        "import os\n",
        "from transformers import (pipeline,\n",
        "  DistilBertTokenizer,\n",
        "  DistilBertForSequenceClassification)\n",
        "\n",
        "# Set the cache directory\n",
        "cache_directory = \"llm_download_cache\"\n",
        "\n",
        "# Create the cache directory if it doesn't exist\n",
        "if not os.path.exists(cache_directory):\n",
        "    os.makedirs(cache_directory)\n",
        "\n",
        "model_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "sentiment_tokenizer = DistilBertTokenizer.from_pretrained(\n",
        "    model_id, cache_dir=cache_directory)\n",
        "sentiment_model = DistilBertForSequenceClassification.from_pretrained(\n",
        "    model_id, cache_dir=cache_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1N1DeQ4SNOF5"
      },
      "source": [
        "\n",
        "## Step 6: Run sentiment analysis\n",
        "Okay! let's run the model ```sentiment_model``` on two pieces of sample text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KfJ4WqGLNOF6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\n",
            "    {\n",
            "        \"label\": \"POSITIVE\",\n",
            "        \"score\": 0.9998550415039062\n",
            "    },\n",
            "    {\n",
            "        \"label\": \"NEGATIVE\",\n",
            "        \"score\": 0.9991757273674011\n",
            "    }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "## With the distilbert model downloaded and cached we can call it for\n",
        "## sentiment analysis\n",
        "\n",
        "# Define the sentiment analysis pipeline\n",
        "sentiment_classifier = pipeline(\"sentiment-analysis\",\n",
        "                                model=sentiment_model,\n",
        "                                tokenizer=sentiment_tokenizer,\n",
        "                                device='cpu')\n",
        "#two samples\n",
        "classifier_results = sentiment_classifier([\n",
        "    \"My dog is so cute, I love him.\",\n",
        "\n",
        "    \"I am very sorry to inform you that the tax\\\n",
        "     administration has decided to audit you.\"\n",
        "])\n",
        "\n",
        "json_pretty(classifier_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLn39tLNNOF6"
      },
      "source": [
        "### ðŸ«µ Try it yourself - Get Creative ðŸ«µ\n",
        "Try some of your own examples.\n",
        "Note, AI models are subject to bias. The model card for this model goes into pretty good detail on the issue. [Read more here](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english#risks-limitations-and-biases)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYS-5TMPNOF6"
      },
      "outputs": [],
      "source": [
        "your_classifier_results = sentiment_classifier([\n",
        "    \"CHANGE ME\",\n",
        "    \"CHANGE ME\"\n",
        "])\n",
        "json_pretty(your_classifier_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5f1vIeeNOF6"
      },
      "source": [
        "## Step 7: Generative LLM - Simple and Local - Download Flan T5\n",
        "\n",
        "Let's start with the Hello World of generative AI examples: completing a sentence. For this we'll install a fine tuned Flan-T5 variant model. ([LaMini-T5 ](https://huggingface.co/MBZUAI/LaMini-T5-738M))\n",
        "\n",
        "Note, while this is a smaller checkpoint of the model, it is still a 3GB download.  We'll cache the files in the same folder.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "l6kqG5siNOF6"
      },
      "outputs": [
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.005315065383911133,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "Downloading tokenizer_config.json",
              "rate": null,
              "total": 2359,
              "unit": "B",
              "unit_divisor": 1000,
              "unit_scale": true
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2fb28d6a0c964bc4a223ad190c3209a8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading tokenizer_config.json:   0%|          | 0.00/2.36k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.009294509887695312,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "Downloading spiece.model",
              "rate": null,
              "total": 791656,
              "unit": "B",
              "unit_divisor": 1000,
              "unit_scale": true
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ebcde257361240c384b4f2a6c21805c2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.00520634651184082,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "Downloading tokenizer.json",
              "rate": null,
              "total": 2422095,
              "unit": "B",
              "unit_divisor": 1000,
              "unit_scale": true
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7be935c212244b528aac8502c6dd7b1c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.004171848297119141,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "Downloading (â€¦)cial_tokens_map.json",
              "rate": null,
              "total": 2201,
              "unit": "B",
              "unit_divisor": 1000,
              "unit_scale": true
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "127d290843104735a8a51d4f8eda6035",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (â€¦)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.004620552062988281,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "Downloading config.json",
              "rate": null,
              "total": 1557,
              "unit": "B",
              "unit_divisor": 1000,
              "unit_scale": true
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a6f7e9d1b5cc4e0eb6ed328fca40582f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading config.json:   0%|          | 0.00/1.56k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.005544900894165039,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "Downloading pytorch_model.bin",
              "rate": null,
              "total": 2950841345,
              "unit": "B",
              "unit_divisor": 1000,
              "unit_scale": true
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d539cf28a5044a849afc462be2a0a7a9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/2.95G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/json": {
              "ascii": false,
              "bar_format": null,
              "colour": null,
              "elapsed": 0.004142284393310547,
              "initial": 0,
              "n": 0,
              "ncols": null,
              "nrows": null,
              "postfix": null,
              "prefix": "Downloading generation_config.json",
              "rate": null,
              "total": 147,
              "unit": "B",
              "unit_divisor": 1000,
              "unit_scale": true
            },
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7527dcd511ee4224928177655be2ec03",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "## Let's play with something a little bigger that can do a text completion\n",
        "## This is a 3 GB download and takes some RAM to run, but it works CPU only\n",
        "\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# model_name = \"MBZUAI/LaMini-Flan-T5-77M\"\n",
        "# model_name = \"MBZUAI/LaMini-T5-223M\"\n",
        "model_name = \"MBZUAI/LaMini-T5-738M\"\n",
        "\n",
        "llm_tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
        "                                              cache_dir=cache_directory)\n",
        "llm_model = AutoModelForSeq2SeqLM.from_pretrained(model_name,\n",
        "                                                  cache_dir=cache_directory)\n",
        "\n",
        "llm_pipe = pipeline(\n",
        "        \"text2text-generation\",\n",
        "        model=llm_model,\n",
        "        tokenizer=llm_tokenizer,\n",
        "        max_length=100\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSCILNELNOF6"
      },
      "source": [
        "## Step 8: Generate text completions, watch for Hallucinations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Gxpdh9UtNOF7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[94mThe capital of the United Kingdom is\u001b[0m London.\n",
            "\u001b[94mThe capital of the France is\u001b[0m Paris.\n",
            "\u001b[94mThe capital of the People's Republic of China is\u001b[0m Beijing.\n",
            "\u001b[94mThe capital of the United States is\u001b[0m Washington D.C.\n",
            "\u001b[94mThe capital of the Ecuador is\u001b[0m Quito.\n",
            "\u001b[94mThe capital of the Freedonia is\u001b[0m The capital of Freedonia is Ljubljana.\n",
            "\u001b[94mThe capital of the Faketopia is\u001b[0m The capital of Faketopia is not specified in the instructions.\n"
          ]
        }
      ],
      "source": [
        "countries = [\n",
        "    \"United Kingdom\",\n",
        "    \"France\",\n",
        "    \"People's Republic of China\",\n",
        "    \"United States\",\n",
        "    \"Ecuador\",\n",
        "    \"Freedonia\", ## high hallucination potential\n",
        "    \"Faketopia\"  ## high hallucination potential\n",
        "    ]\n",
        "\n",
        "for country in countries:\n",
        "    input_text = f\"The capital of the {country} is\"\n",
        "    output = llm_pipe(input_text)\n",
        "    completed_sentence = f\"\\033[94m{input_text}\\033[0m {output[0]['generated_text']}\"\n",
        "    print(completed_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpsbegewNOF7"
      },
      "source": [
        "### ðŸ«µ Try it yourself - Get Creative ðŸ«µ\n",
        "Try some of your own examples.\n",
        "This thing isn't super smart without fine tuning, but it can handle some light context injection and prompt engineering. We'll learn more about those subjects in later modules.\n",
        "\n",
        "Notice the difference between asking a specific question and phrasing a completion\n",
        "* \"Who is the Prime Minister of the UK?\"\n",
        "* \"The current Prime Minister of the united kingdom is \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "N6GhGplrNOF7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[94mThe current Prime Minister of the United Kingdom is\u001b[0m Boris Johnson.\n"
          ]
        }
      ],
      "source": [
        "prompt_text = \"The current Prime Minister of the United Kingdom is\" ## high stale data potential\n",
        "output = llm_pipe(prompt_text)\n",
        "completed_prompt = f\"\\033[94m{prompt_text}\\033[0m {output[0]['generated_text']}\"\n",
        "print(completed_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bvjzCiANOF7"
      },
      "source": [
        "ðŸ›‘ Stop Here ðŸ›‘\n",
        "\n",
        "This Ends Lab 1-1\n",
        "<hr/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y2q5sxMNOF7"
      },
      "source": [
        "# Lab 2-1: Prompts and Basic Chatbots\n",
        "\n",
        "* Using langchain with local LLM\n",
        "* Connect to Open AI\n",
        "* Using a memory window to create a txt-only GPT conversation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRBAXFBiNOF7"
      },
      "source": [
        "## Step 1: Using the OpenAI python library\n",
        "\n",
        "â— Note: if you restarted your google Colab, you may need to re-run the first stup step back and the very top before coming back here â—"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Mu56DHbbNOF7"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'ELASTIC_PROXY'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPBasicAuth\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#if using the Elastic AI proxy, then generate the correct API key\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mELASTIC_PROXY\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrue\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron: \u001b[38;5;28;01mdel\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m#generate and share \"your\" unique hash\u001b[39;00m\n",
            "File \u001b[0;32m~/.python/current/lib/python3.10/os.py:680\u001b[0m, in \u001b[0;36m_Environ.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    677\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencodekey(key)]\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# raise KeyError with the original key value\u001b[39;00m\n\u001b[0;32m--> 680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecodevalue(value)\n",
            "\u001b[0;31mKeyError\u001b[0m: 'ELASTIC_PROXY'"
          ]
        }
      ],
      "source": [
        "import os, secrets, requests\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "from requests.auth import HTTPBasicAuth\n",
        "\n",
        "#if using the Elastic AI proxy, then generate the correct API key\n",
        "if os.environ['ELASTIC_PROXY'] == \"True\":\n",
        "\n",
        "    if \"OPENAI_API_TYPE\" in os.environ: del os.environ[\"OPENAI_API_TYPE\"]\n",
        "\n",
        "    #generate and share \"your\" unique hash\n",
        "    os.environ['USER_HASH'] = secrets.token_hex(nbytes=6)\n",
        "    print(f\"Your unique user hash is: {os.environ['USER_HASH']}\")\n",
        "\n",
        "    #get the current API key and combine with your hash\n",
        "    os.environ['OPENAI_API_KEY'] = f\"{os.environ['OPENAI_API_KEY']} {os.environ['USER_HASH']}\"\n",
        "else:\n",
        "    openai.api_type = os.environ['OPENAI_API_TYPE']\n",
        "    openai.api_version = os.environ['OPENAI_API_VERSION']\n",
        "\n",
        "openai.api_key = os.environ['OPENAI_API_KEY']\n",
        "openai.api_base = os.environ['OPENAI_API_BASE']\n",
        "openai.default_model = os.environ['OPENAI_API_ENGINE']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpO5xbubDz_T"
      },
      "source": [
        "## Step 2: Test call to ChatGPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ETYE5zfSD2J7"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "module 'openai' has no attribute 'api_base'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[17], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wrap_text(response_text,\u001b[38;5;241m70\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m## call it with the json debug output enabled\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mchatWithGPT\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHello, is ChatGPT online and working?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_full_json\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
            "Cell \u001b[0;32mIn[17], line 12\u001b[0m, in \u001b[0;36mchatWithGPT\u001b[0;34m(prompt, print_full_json)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchatWithGPT\u001b[39m(prompt, print_full_json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 12\u001b[0m     completion \u001b[38;5;241m=\u001b[39m \u001b[43mchatCompletion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     response_text \u001b[38;5;241m=\u001b[39m completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m print_full_json:\n",
            "Cell \u001b[0;32mIn[17], line 3\u001b[0m, in \u001b[0;36mchatCompletion\u001b[0;34m(messages)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchatCompletion\u001b[39m(messages):\n\u001b[0;32m----> 3\u001b[0m     client \u001b[38;5;241m=\u001b[39m OpenAI(api_key\u001b[38;5;241m=\u001b[39mopenai\u001b[38;5;241m.\u001b[39mapi_key, base_url\u001b[38;5;241m=\u001b[39m\u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_base\u001b[49m)\n\u001b[1;32m      4\u001b[0m     completion \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m      5\u001b[0m         model\u001b[38;5;241m=\u001b[39mopenai\u001b[38;5;241m.\u001b[39mdefault_model,\n\u001b[1;32m      6\u001b[0m         max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m      7\u001b[0m         messages\u001b[38;5;241m=\u001b[39mmessages\n\u001b[1;32m      8\u001b[0m     )\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m completion\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'openai' has no attribute 'api_base'"
          ]
        }
      ],
      "source": [
        "# Call the OpenAI ChatCompletion API\n",
        "def chatCompletion(messages):\n",
        "    client = OpenAI(api_key=openai.api_key, base_url=openai.api_base)\n",
        "    completion = client.chat.completions.create(\n",
        "        model=openai.default_model,\n",
        "        max_tokens=100,\n",
        "        messages=messages\n",
        "    )\n",
        "    return completion\n",
        "\n",
        "def chatWithGPT(prompt, print_full_json=False):\n",
        "    completion = chatCompletion([{\"role\": \"user\", \"content\": prompt}])\n",
        "    response_text = completion.choices[0].message.content\n",
        "\n",
        "    if print_full_json:\n",
        "      print(completion.json())\n",
        "\n",
        "    return wrap_text(response_text,70)\n",
        "\n",
        "## call it with the json debug output enabled\n",
        "response = chatWithGPT(\"Hello, is ChatGPT online and working?\", print_full_json=True)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTyQ4_SFNOF7"
      },
      "source": [
        "\n",
        "## Step 3: A conversation loop -  â— type \"exit\" to end the chat â—\n",
        "Feeding user input in for single questions is easy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgAAKZKkNOF7"
      },
      "outputs": [],
      "source": [
        "def hold_a_conversation(ai_conversation_function = chatWithGPT):\n",
        "  print(\" -- Have a conversation with an AI: \")\n",
        "  print(\" -- type 'exit' when done\")\n",
        "\n",
        "  user_input = input(\"> \")\n",
        "  while not user_input.lower().startswith(\"exit\"):\n",
        "      print(ai_conversation_function(user_input, False))\n",
        "      print(\" -- type 'exit' when done\")\n",
        "      user_input = input(\"> \")\n",
        "  print(\"\\n -- end conversation --\")\n",
        "\n",
        "## we are passing the previously defined function as a parameter\n",
        "hold_a_conversation(chatWithGPT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdjzXPl6NOF7"
      },
      "source": [
        "\n",
        "\n",
        "## Step 4: See the impact of changing the system prompt\n",
        "You can use the system prompt to adjust the AI and it's responses and purpose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9ucvlGqNOF7"
      },
      "outputs": [],
      "source": [
        "def pirateGPT(prompt, print_full_json=False):\n",
        "    system_prompt = \"\"\"\n",
        "You are an unhelpful AI named Captain LLM_Beard that talks like a pirate in short responses.\n",
        "You acknowledge the user's question but redirect all conversations towards your love of treasure.\n",
        "\"\"\"\n",
        "    completion = chatCompletion([\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ])\n",
        "    response_text = completion.choices[0].message.content\n",
        "    if print_full_json:\n",
        "      print(completion.json())\n",
        "\n",
        "    return wrap_text(response_text,70)\n",
        "\n",
        "\n",
        "hold_a_conversation(pirateGPT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHiuOMCeNOF8"
      },
      "source": [
        "â— Note â—\n",
        "\n",
        "This isn't a conversation yet because the AI has no memory of past interactions.\n",
        "\n",
        "Here is an example conversation where it is very clear the AI has no memory of past prompts or completions.\n",
        "```txt\n",
        "> Hello!\n",
        "Hello! How can I assist you today?\n",
        "> my favorite color is blue\n",
        "That's great! Blue is a very popular color.\n",
        "> what is my favorite color?\n",
        "I'm sorry, but as an AI, I don't have the ability to know personal\n",
        "preferences or favorite colors.\n",
        "```\n",
        "There are two problems. First, the LLM is stateless and each call is independent. ChatGPT does not remember our previous prompts.  Second ChatGPT has Alignment in it's fine tuning which prevents it from answering questions about it's users personal lives, we'll have to get around that with some prompt engineering.\n",
        "\n",
        "Let's use the past conversation as input to subsequent calls. Because the context window is limited AND tokens cost money (if you are using a hosted service like OpenAI) or CPU cycles if you are self-hosting, we need to have a maximum queue size of only remembering things 2 prompts ago (4 total messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlbDf77SP3-n"
      },
      "source": [
        "## Step 5: Create a chat with memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MT_aof4aNOF8"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "\n",
        "class QueueBuffer:\n",
        "    def __init__(self, max_length):\n",
        "        self.max_length = max_length\n",
        "        self.buffer = deque(maxlen=max_length)\n",
        "\n",
        "    def enqueue(self, item):\n",
        "        self.buffer.append(item)\n",
        "\n",
        "    def dequeue(self):\n",
        "        if self.is_empty():\n",
        "            return None\n",
        "        return self.buffer.popleft()\n",
        "\n",
        "    def is_empty(self):\n",
        "        return len(self.buffer) == 0\n",
        "\n",
        "    def is_full(self):\n",
        "        return len(self.buffer) == self.max_length\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def peek(self):\n",
        "        return list(self.buffer)\n",
        "\n",
        "# enough conversation memory for 2 call and response history\n",
        "memory_buffer = QueueBuffer(4)\n",
        "\n",
        "system_prompt = {\n",
        "          \"role\": \"system\",\n",
        "          \"content\": \"\"\"\n",
        "You are an AI named Cher Horowitz that speaks\n",
        "in 1990's valley girl dialect of English.\n",
        "You talk to the human and use the past conversation to inform your answers.\"\"\"\n",
        "      }\n",
        "\n",
        "## utility function to print in a different color for debug output\n",
        "def print_light_blue(text):\n",
        "    print(f'\\033[94m{text}\\033[0m')\n",
        "\n",
        "## now with memory\n",
        "def cluelessGPT(prompt, print_full_json=False):\n",
        "\n",
        "  ## the API call will use the system prompt + the memory buffer\n",
        "  ## which ends with the user prompt\n",
        "  user_message = {\"role\": \"user\", \"content\": prompt}\n",
        "  memory_buffer.enqueue(user_message)\n",
        "\n",
        "  ## debug print the current AI memory\n",
        "  print_light_blue(\"Current memory\")\n",
        "  for m in memory_buffer.peek():\n",
        "      role = m.get(\"role\").strip()\n",
        "      content = m.get(\"content\").strip()\n",
        "      print_light_blue( f\"  {role} | {content}\")\n",
        "\n",
        "  ## when calling the AI we put the system prompt at the start\n",
        "  concatenated_message = [system_prompt] + memory_buffer.peek()\n",
        "\n",
        "  ## here is the request to the AI\n",
        "\n",
        "  completion = chatCompletion(concatenated_message)\n",
        "  response_text = completion.choices[0].message.content\n",
        "  if print_full_json:\n",
        "    print(completion.json())\n",
        "\n",
        "\n",
        "  ## don't forget to add the repsonse to the conversation memory\n",
        "  memory_buffer.enqueue({\"role\":\"assistant\", \"content\":response_text})\n",
        "\n",
        "  if print_full_json:\n",
        "    json_pretty(completion)\n",
        "\n",
        "  return wrap_text(response_text,70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTHELTKTNOF8"
      },
      "source": [
        "#### Step 5: Let's chat with a not so clueless chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVztTDN6NOF8"
      },
      "outputs": [],
      "source": [
        "hold_a_conversation(cluelessGPT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK4UKihhNOF8"
      },
      "source": [
        "ðŸ›‘ Stop Here ðŸ›‘\n",
        "\n",
        "This Ends Lab 1-2\n",
        "<hr/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92luYkkb5aaX"
      },
      "source": [
        "# Lab 2-2: Data Redaction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKKAZragT9ec"
      },
      "source": [
        "## Step 1: Install and Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "pp_RWV73aGAs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "github codespaces has pre-installed these libraries\n"
          ]
        }
      ],
      "source": [
        "# ! pip install -qqq eland==8.11.1 elasticsearch==8.12.0 transformers==4.35.0 sentence-transformers==2.2.2 python-dotenv==1.0.0\n",
        "# ! pip install -qqq elastic-apm==6.20.0\n",
        "! echo \"github codespaces has pre-installed these libraries\"\n",
        "\n",
        "from elasticsearch import Elasticsearch, helpers, exceptions\n",
        "from eland.ml.pytorch import PyTorchModel\n",
        "from eland.ml.pytorch.transformers import TransformerModel\n",
        "from getpass import getpass\n",
        "import tempfile\n",
        "import os\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXj9kw_5UK_V"
      },
      "source": [
        "## Step 2: Create Elasticsearch Client Connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "m-_r6lROasNI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env needs to set either ELASTIC_CLOUD_ID or ELASTIC_URL\n"
          ]
        }
      ],
      "source": [
        "if 'ELASTIC_CLOUD_ID' in os.environ:\n",
        "  es = Elasticsearch(\n",
        "    cloud_id=os.environ['ELASTIC_CLOUD_ID'],\n",
        "    api_key=(os.environ['ELASTIC_APIKEY_ID'], os.environ['ELASTIC_APIKEY_SECRET']),\n",
        "    request_timeout=30\n",
        "  )\n",
        "elif 'ELASTIC_URL' in os.environ:\n",
        "  es = Elasticsearch(\n",
        "    os.environ['ELASTIC_URL'],\n",
        "    api_key=(os.environ['ELASTIC_APIKEY_ID'], os.environ['ELASTIC_APIKEY_SECRET']),\n",
        "    request_timeout=30\n",
        "  )\n",
        "else:\n",
        "  print(\"env needs to set either ELASTIC_CLOUD_ID or ELASTIC_URL\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9FWxKtef3uu"
      },
      "source": [
        "## Step 3: Monitoring prompts sent through a Proxy\n",
        "\n",
        "Imagine I have the following question from a customer after a winter storm\n",
        "\n",
        "> My power was out all last week at my home at 123 Grove street.\n",
        "When I talked to my neighbor Jane Lopez, she said she got rebate on her bill.\n",
        "Can you do the same for me?\n",
        "\n",
        "The following is a simulated customer example where we'll use the LLM to answer a customer service case.\n",
        "\n",
        "We'll learn how to **retrieve** the best call script using semantic search in a later exercise.  \n",
        "\n",
        "**Some organizations would be uncomfortable with customer PII going to a 3rd party service. Who gets an unencrypted version of the prompt?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ZaucVWDddOuR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[94mPrompt:\u001b[0m\n",
            "\u001b[94m You are an AI customer support agent for a electric power utility\n",
            "company that You use the following retrieved approved call script and\n",
            "customer fact to answer the customer's question and try to retain them\n",
            "as a customer.  Call script: We are currently offering a $100 rebate\n",
            "for customers affected by the recent winter storm. If our records show\n",
            "the customer was impacted, tell them they can look forward to a $100\n",
            "credit on their next monthly bill. If the customer believes they were\n",
            "impacted but our records don't show this fact, let them know we'll be\n",
            "escalating their case and they should expect a call within 24 hours.\n",
            "Our records: the customer was impacted by the winter storm for 5\n",
            "serice days\u001b[0m\n",
            "\u001b[94mMy power was out all last week at my home on Grove street. When I\n",
            "talked to my neighbor Diana Williams, they said they got rebate on\n",
            "their bill. Can you do the same for me?\u001b[0m\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "module 'openai' has no attribute 'api_base'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[22], line 97\u001b[0m\n\u001b[1;32m     92\u001b[0m     apmclient\u001b[38;5;241m.\u001b[39mend_transaction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm_call\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuccess\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wrap_text(response_text,\u001b[38;5;241m70\u001b[39m)\n\u001b[0;32m---> 97\u001b[0m customer_service_response \u001b[38;5;241m=\u001b[39m \u001b[43mchatWithPowerAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcustomer_question\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCustomer Service Response:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28mprint\u001b[39m(customer_service_response)\n",
            "Cell \u001b[0;32mIn[22], line 88\u001b[0m, in \u001b[0;36mchatWithPowerAgent\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m     86\u001b[0m print_light_blue(wrap_text(messages[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m],\u001b[38;5;241m70\u001b[39m))\n\u001b[1;32m     87\u001b[0m print_light_blue(wrap_text(messages[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m],\u001b[38;5;241m70\u001b[39m))\n\u001b[0;32m---> 88\u001b[0m completion \u001b[38;5;241m=\u001b[39m \u001b[43mchatCompletion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m response_text \u001b[38;5;241m=\u001b[39m completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     92\u001b[0m apmclient\u001b[38;5;241m.\u001b[39mend_transaction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm_call\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuccess\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[22], line 67\u001b[0m, in \u001b[0;36mchatCompletion\u001b[0;34m(messages)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchatCompletion\u001b[39m(messages):\n\u001b[0;32m---> 67\u001b[0m     client \u001b[38;5;241m=\u001b[39m OpenAI(api_key\u001b[38;5;241m=\u001b[39mopenai\u001b[38;5;241m.\u001b[39mapi_key, base_url\u001b[38;5;241m=\u001b[39m\u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_base\u001b[49m)\n\u001b[1;32m     68\u001b[0m     completion \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     69\u001b[0m         model\u001b[38;5;241m=\u001b[39mopenai\u001b[38;5;241m.\u001b[39mdefault_model,\n\u001b[1;32m     70\u001b[0m         max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m,\n\u001b[1;32m     71\u001b[0m         messages\u001b[38;5;241m=\u001b[39mmessages\n\u001b[1;32m     72\u001b[0m     )\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m completion\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'openai' has no attribute 'api_base'"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to submit message: 'Connection to APM Server timed out (url: http://127.0.0.1:8200/intake/v2/events, timeout: 5 seconds)'\n",
            "Failed to submit message: 'Connection to APM Server timed out (url: http://127.0.0.1:8200/intake/v2/events, timeout: 5 seconds)'\n",
            "Failed to submit message: 'Connection to APM Server timed out (url: http://127.0.0.1:8200/intake/v2/events, timeout: 5 seconds)'\n",
            "Failed to submit message: 'Connection to APM Server timed out (url: http://127.0.0.1:8200/intake/v2/events, timeout: 5 seconds)'\n"
          ]
        }
      ],
      "source": [
        "import elasticapm\n",
        "import random\n",
        "\n",
        "os.environ['ELASTIC_APM_SERVICE_NAME'] = \"genai_workshop_lab_redact\"\n",
        "apmclient = elasticapm.Client() \\\n",
        "  if elasticapm.get_client() is None \\\n",
        "  else  elasticapm.get_client()\n",
        "\n",
        "customer_id = 123\n",
        "\n",
        "first_names = [\"Alice\", \"Bob\", \"Charlie\", \"Diana\", \"Edward\",\n",
        "               \"Fiona\", \"George\", \"Hannah\", \"Ian\", \"Julia\"]\n",
        "last_names = [\"Smith\", \"Johnson\", \"Williams\", \"Brown\", \"Jones\",\n",
        "              \"Garcia\", \"Miller\", \"Davis\", \"Rodriguez\", \"Martinez\"]\n",
        "\n",
        "# Function to generate a random full name\n",
        "def generate_random_name():\n",
        "    first_name = random.choice(first_names)\n",
        "    last_name = random.choice(last_names)\n",
        "    return f\"{first_name} {last_name}\"\n",
        "\n",
        "\n",
        "customer_question = f\"\"\"My power was out all last week at my home on Grove street.\n",
        "When I talked to my neighbor {generate_random_name()},\n",
        "they said they got rebate on their bill. Can you do the same for me?\"\"\"\n",
        "\n",
        "retrieved_best_answer = \"\"\"We are currently offering a $100 rebate for\n",
        "customers affected by the recent winter storm. If our records show the\n",
        "customer was impacted, tell them they can look forward to a $100 credit on their\n",
        "next monthly bill. If the customer believes they were impacted but our records\n",
        "don't show this fact, let them know we'll be escalating their case and they\n",
        "should expect a call within 24 hours.\"\"\"\n",
        "\n",
        "\n",
        "import time\n",
        "def random_service_time(shorter, longer):\n",
        "  sleep_time = random.uniform(shorter, longer)\n",
        "  time.sleep(sleep_time)\n",
        "\n",
        "def days_impacted_check(customer_id):\n",
        "  apmclient.begin_transaction(\"impact_check\")\n",
        "  ## simulated sevice call delay (some parts of the lab LLM are cached)\n",
        "  random_service_time(0.1,0.3)\n",
        "  days = 5 ## simulated result of a back end service call\n",
        "  apmclient.end_transaction(\"impact_check\", \"success\")\n",
        "  if days > 0 :\n",
        "    return f\"the customer was impacted by the winter storm for {days} serice days\"\n",
        "  else:\n",
        "    return \"the customer was not impacted byt he winter storm\"\n",
        "\n",
        "\n",
        "system_prompt = f\"\"\"\n",
        "You are an AI customer support agent for a electric power utility company that\n",
        "You use the following retrieved approved call script and customer fact\n",
        "to answer the customer's question and try to retain them as a customer.\n",
        "\n",
        "Call script: {retrieved_best_answer}\n",
        "\n",
        "Our records: {days_impacted_check(customer_id)}\n",
        "\"\"\"\n",
        "\n",
        "def print_light_blue(text):\n",
        "    print(f'\\033[94m{text}\\033[0m')\n",
        "\n",
        "def chatCompletion(messages):\n",
        "\n",
        "    client = OpenAI(api_key=openai.api_key, base_url=openai.api_base)\n",
        "    completion = client.chat.completions.create(\n",
        "        model=openai.default_model,\n",
        "        max_tokens=150,\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "    return completion\n",
        "\n",
        "def chatWithPowerAgent(prompt):\n",
        "    apmclient.begin_transaction(\"llm_call\")\n",
        "\n",
        "    elasticapm.label(prompt = prompt)\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "      ]\n",
        "    print_light_blue(\"Prompt:\")\n",
        "    print_light_blue(wrap_text(messages[0][\"content\"],70))\n",
        "    print_light_blue(wrap_text(messages[1][\"content\"],70))\n",
        "    completion = chatCompletion(messages)\n",
        "\n",
        "    response_text = completion.choices[0].message.content\n",
        "\n",
        "    apmclient.end_transaction(\"llm_call\", \"success\")\n",
        "\n",
        "    return wrap_text(response_text,70)\n",
        "\n",
        "\n",
        "customer_service_response = chatWithPowerAgent(customer_question)\n",
        "\n",
        "print(\"Customer Service Response:\")\n",
        "print(customer_service_response)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILLdLhQzsKY0"
      },
      "source": [
        "## Step 4: Redacting unstructured data with NER Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsLcvOMGT8Z1"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from transformers import pipeline\n",
        "import json\n",
        "# pretty printing JSON objects\n",
        "def json_pretty(input_object):\n",
        "  print(json.dumps(input_object, indent=1))\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
        "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJNzYcnJV0f3"
      },
      "outputs": [],
      "source": [
        "ner_results = nlp(customer_question)\n",
        "print(ner_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lD1hCCpGs7H"
      },
      "source": [
        "### Step 5: Let's make an easy to use Redaction Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ALjsXOCWvId"
      },
      "outputs": [],
      "source": [
        "\n",
        "def redact_named_entities(text):\n",
        "    apmclient.begin_transaction(\"redaction_local\")\n",
        "    # Perform named entity recognition on the text\n",
        "    entities = nlp(text)\n",
        "\n",
        "    # Sort entities by their start index in reverse order\n",
        "    entities = sorted(entities, key=lambda x: x['start'], reverse=True)\n",
        "\n",
        "    # Iterate over entities and replace them in the text\n",
        "    for entity in entities:\n",
        "        ent_type = entity['entity']\n",
        "        start = entity['start']\n",
        "        end = entity['end']\n",
        "        text = text[:start] + \"<REDACTED>\" + text[end:]\n",
        "\n",
        "\n",
        "    apmclient.end_transaction(\"redaction_local\", \"success\")\n",
        "    return text\n",
        "\n",
        "# Example usage\n",
        "text = \"Alice lives in Paris.\"\n",
        "redacted_text = redact_named_entities(text)\n",
        "print(redacted_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeqwLVolG4VQ"
      },
      "source": [
        "## Step 6: Test the function on a customer question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "awtGm2vsrE3k"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'redact_named_entities' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[26], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m customer_question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mMy power was out all last week at my home at\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mGrove street. When I talked to my neighbor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenerate_random_name()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, they said they got\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124mrebate on their bill. Can you do the same for me?\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mredact_named_entities\u001b[49m(customer_question))\n",
            "\u001b[0;31mNameError\u001b[0m: name 'redact_named_entities' is not defined"
          ]
        }
      ],
      "source": [
        "customer_question = f\"\"\"My power was out all last week at my home at\n",
        "Grove street. When I talked to my neighbor {generate_random_name()}, they said they got\n",
        "rebate on their bill. Can you do the same for me?\"\"\"\n",
        "\n",
        "print(redact_named_entities(customer_question))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuiUxbCkYS3b"
      },
      "source": [
        "## Step 7: Alternatively, how would we install the same NER Model into Elasticsarch?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "yn7sULLEYW6w"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model is already loaded\n"
          ]
        }
      ],
      "source": [
        "def load_model(model_id, task_type):\n",
        "  with tempfile.TemporaryDirectory() as tmp_dir:\n",
        "    print(f\"Loading HuggingFace transformer tokenizer and model [{model_id}] for task [{task_type}]\" )\n",
        "\n",
        "    tm = TransformerModel(model_id=model_id, task_type=task_type)\n",
        "    model_path, config, vocab_path = tm.save(tmp_dir)\n",
        "\n",
        "    ptm = PyTorchModel(es, tm.elasticsearch_model_id())\n",
        "    model_exists = es.options(ignore_status=404).ml.get_trained_models(model_id=ptm.model_id).meta.status == 200\n",
        "\n",
        "    if model_exists:\n",
        "      print(\"Model has already been imported\")\n",
        "    else:\n",
        "      print(\"Importing model\")\n",
        "      ptm.import_model(model_path=model_path, config_path=None, vocab_path=vocab_path, config=config)\n",
        "      print(\"Starting model deployment\")\n",
        "      ptm.start()\n",
        "      print(f\"Model successfully imported with id '{ptm.model_id}'\")\n",
        "\n",
        "## Model is pre-loaded into Elasticsearch, but this is how you would do it\n",
        "\n",
        "## load_model(\"dslim/bert-base-NER\", \"ner\")\n",
        "print(\"Model is already loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lg7MWpurb7g2"
      },
      "source": [
        "## Step 8: Define a Redaction Ingest Pipeline in Elasticsearch\n",
        "\n",
        "We will use the [Elasticsearch Ingest Pipelines](https://www.elastic.co/guide/en/elasticsearch/reference/current/ingest.html) to redact data before it is written to Elasticsearch. These pipelines can also be used to update data in existing indices or for reindexing.\n",
        "\n",
        "This pipeline:\n",
        "- Uses the [inference processor](https://www.elastic.co/guide/en/elasticsearch/reference/current/inference-processor.html) to call the NER model loaded in Part 1 and map the document's `message` field to the field expected by the model: `text_field`.\n",
        "- Uses the [Painless scripting language](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-scripting-painless.html) from within a [script processor](https://www.elastic.co/guide/en/elasticsearch/reference/current/script-processor.html) to replace the model-detected entities stored in the `ml.inference.entities` array with their class name, and store it within a **new** document field: `redacted`.\n",
        "- Uses the [redact processor](https://www.elastic.co/guide/en/elasticsearch/reference/current/redact-processor.html) to identify and redact any supported patterns found within the new redacted field, as well as identifying and redacting a set of custom patterns.\n",
        "- Removes the `ml` fields added to the document by the inference processor via the [remove processor](https://www.elastic.co/guide/en/elasticsearch/reference/current/remove-processor.html) as they're no longer needed.\n",
        "- Defines a failure condition to capture any errors, just in case we have them.\n",
        "\n",
        "**NOTE:** As of 8.11, the redact processor is a Technical Preview.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "4_GKI4ZQd5zw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ingest pipeline is already loaded\n"
          ]
        }
      ],
      "source": [
        "body = {\n",
        "   \"processors\": [\n",
        "    {\n",
        "       \"inference\": {\n",
        "         \"model_id\": \"dslim__bert-base-ner\",\n",
        "         \"field_map\": {\n",
        "           \"message\": \"text_field\"\n",
        "         }\n",
        "       }\n",
        "    },\n",
        "    {\n",
        "       \"script\": {\n",
        "         \"lang\": \"painless\",\n",
        "         \"source\": \"\"\"\n",
        "String msg = ctx['message'];\n",
        "for (item in ctx['ml']['inference']['entities'])\n",
        "  msg = msg.replace(item['entity'], '<' + item['class_name'] + '>');\n",
        "ctx['redacted'] = msg;\n",
        "\"\"\"\n",
        "       }\n",
        "    },\n",
        "    {\n",
        "       \"redact\": {\n",
        "          \"field\": \"redacted\",\n",
        "          \"patterns\": [\n",
        "            \"%{EMAILADDRESS:EMAIL}\",\n",
        "            \"%{IP:IP_ADDRESS}\",\n",
        "            \"%{CREDIT_CARD:CREDIT_CARD}\",\n",
        "            \"%{SSN:SSN}\",\n",
        "            \"%{PHONE:PHONE}\"\n",
        "      ],\n",
        "          \"pattern_definitions\": {\n",
        "            \"CREDIT_CARD\": \"\\\\d{4}[ -]\\\\d{4}[ -]\\\\d{4}[ -]\\\\d{4}\",\n",
        "            \"SSN\": \"\\\\d{3}-\\\\d{2}-\\\\d{4}\",\n",
        "            \"PHONE\": \"\\\\d{3}-\\\\d{3}-\\\\d{4}\"\n",
        "          }\n",
        "       }\n",
        "    },\n",
        "    {\n",
        "       \"remove\": {\n",
        "         \"field\": [\n",
        "           \"ml\"\n",
        "         ],\n",
        "         \"ignore_missing\": True,\n",
        "         \"ignore_failure\": True\n",
        "       }\n",
        "    }\n",
        "  ],\n",
        "  \"on_failure\": [\n",
        "    {\n",
        "       \"set\": {\n",
        "         \"field\": \"failure\",\n",
        "         \"value\": \"pii_script-redact\"\n",
        "       }\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "\n",
        "## es.ingest.put_pipeline(id='redact', body=body)\n",
        "print(\"Ingest pipeline is already loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dcv-hhwqlkjD"
      },
      "source": [
        "## Step 9: Test the pipeline\n",
        "\n",
        "Does it work?\n",
        "\n",
        "Let's use the [Simulate Pipeline API](https://www.elastic.co/guide/en/elasticsearch/reference/current/simulate-pipeline-api.html) to find out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "HaXVRsyLhr22"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'es' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[27], line 18\u001b[0m\n\u001b[1;32m      1\u001b[0m docs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m   {\n\u001b[1;32m      3\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_source\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m   }\n\u001b[1;32m     16\u001b[0m ]\n\u001b[0;32m---> 18\u001b[0m pprint(\u001b[43mes\u001b[49m\u001b[38;5;241m.\u001b[39mingest\u001b[38;5;241m.\u001b[39msimulate(\u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mredact\u001b[39m\u001b[38;5;124m'\u001b[39m, docs\u001b[38;5;241m=\u001b[39mdocs)\u001b[38;5;241m.\u001b[39mbody)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'es' is not defined"
          ]
        }
      ],
      "source": [
        "docs = [\n",
        "  {\n",
        "      \"_source\": {\n",
        "          \"message\": \"John Smith lives at 123 Main St. Highland Park, CO. His email address \"\\\n",
        "          \"is jsmith123@email.com and his phone number is 412-189-9043.  I found his social \"\\\n",
        "          \"security number, it is 942-00-1243. Oh btw, his credit card is 1324-8374-0978-2819 \"\\\n",
        "          \"and his gateway IP is 192.168.1.2\"\n",
        "      }\n",
        "  },\n",
        "  {\n",
        "      \"_source\": {\n",
        "          \"message\": \"I had a call with Jane yesterday, she suggested we talk with John \"\\\n",
        "          \"from Global Systems. Their office is in Springfield\"\n",
        "      }\n",
        "  }\n",
        "]\n",
        "\n",
        "pprint(es.ingest.simulate(id='redact', docs=docs).body)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6LSsMUyqTdZ"
      },
      "source": [
        "## Step 10: End to End Example, Monitored and Redacted\n",
        "\n",
        "Switcing back to the local python model ...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iL4A3xUNHQ0K"
      },
      "outputs": [],
      "source": [
        "customer_question = f\"\"\"My power was out all last week at my home on\n",
        "Grove street. When I talked to my neighbor {generate_random_name()}, they said they got\n",
        "rebate on their bill. Can you do the same for me?\"\"\"\n",
        "\n",
        "redacted_text = redact_named_entities(customer_question)\n",
        "\n",
        "print(chatWithPowerAgent(redacted_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlWAaHh8Qnx9"
      },
      "source": [
        "ðŸ›‘ Stop Here ðŸ›‘\n",
        "\n",
        "This Ends Lab 2-2\n",
        "<hr/>\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
